# -*- coding: utf-8 -*-import torch.nn.functional as Fimport torchclass BaseProcessor(object):    def __init__(self):        super(BaseProcessor, self).__init__()    def __call__(self, inputs, outputs, labels, idx):        return self.class2one_hot(outputs,labels)    def class2one_hot(self, outputs, labels):        class_mask = outputs.new_zeros(outputs.size())        label_ids = labels.view(-1, 1)        class_mask.scatter_(1, label_ids, 1.)        return class_maskclass LabelProcessor(BaseProcessor):    def __init__(self, Num, alpha=0.01):        super(LabelProcessor, self).__init__()        self.alpha = alpha        self.base_p = 0.5        self.N = Num        self.soft_label = None        self.output_record = False        self.is_record = False        self.is_first = True    def __call__(self, inputs, outputs, labels, idx):        one_hot_label = self.class2one_hot(outputs, labels)        if self.soft_label is None:            self.soft_label = inputs.new_zeros((self.N, outputs.size(1)))        if self.is_first == True:            self.soft_label[idx, :] = one_hot_label        self.soft_label[idx, :] = \            ((1. - self.alpha) * self.soft_label[idx, :] + \             self.alpha * F.softmax(outputs, dim=1)).detach()        self.soft_label[idx, :] = \            (1. - self.base_p) * self.soft_label[idx, :] + \            self.base_p * one_hot_label        return self.soft_label[idx, :]class LabelProcessor_select(BaseProcessor):    def __init__(self, Num, alpha=0.01):        super(LabelProcessor_select, self).__init__()        self.alpha = alpha        self.base_p = 0.5        self.N = Num        self.soft_label = None        self.output_record = False        self.is_record = False        self.is_first = True    def __call__(self, inputs, outputs, labels, idx):        if self.soft_label is None:             self.soft_label = inputs.new_zeros((self.N, outputs.size(1)))        if self.is_first == True:            one_hot_label = self.class2one_hot(outputs, labels)            self.soft_label[idx, :] = one_hot_label        one_hot_label = self.class2one_hot(outputs, labels)        _, predicted = torch.max(outputs.data, 1)        index = (predicted == labels).view(-1,1).type(torch.cuda.FloatTensor)        select_soft = (1-index)*one_hot_label+index*F.softmax(outputs, dim=1).detach()        self.soft_label[idx, :] = \            (1. - self.alpha) * self.soft_label[idx, :] + self.alpha * select_soft        self.soft_label[idx, :] = \            (1. - self.base_p) * self.soft_label[idx, :] + \            self.base_p * one_hot_label        return self.soft_label[idx, :]class Sample_label_ensemble(object):    def __init__(self, Num, class_num,alpha=0.01, base_p = 0.5,is_select=False):        self.alpha = alpha        self.base_p = base_p        self.N = Num        self.class_num = class_num        self.emsemble_label = None        self.targets = -1*torch.ones(self.N).type(torch.cuda.LongTensor)        self.output_record = False        self.is_record = False        self.is_first = True        self.is_select = is_select        self.number = torch.zeros(self.N, 1).cuda()        self.result = torch.zeros(self.N, class_num).cuda()    def class2one_hot(self, outputs, labels):        class_mask = outputs.new_zeros(outputs.size())        label_ids = labels.view(-1, 1)        class_mask.scatter_(1, label_ids, 1.)        return class_mask    def reset(self):        self.number = torch.zeros(self.N, 1).cuda()        self.result = torch.zeros(self.N, self.class_num).cuda()    def append(self,output,target,idx):        with torch.no_grad():            self.targets[idx] =  target            soft_logit = torch.nn.functional.softmax(output, dim=1)            one_hot_label = self.class2one_hot(output, target)        if self.is_select == True:            _, predicted = torch.max(output.data, 1)            index = (predicted == target).view(-1, 1).type(torch.cuda.FloatTensor)            soft_logit = (1 - index) * one_hot_label + index * soft_logit.detach()        self.result[idx,:]+= soft_logit        self.number[idx, :]+= 1    def update(self):        with torch.no_grad():            index = (self.number != 0)            index2 = index.view(1, -1).squeeze()            one_hot= self.class2one_hot(self.emsemble_label,self.targets).cuda()            newlabel = self.p * one_hot[index2, :].cuda() + (1 - self.p) * self.result[index2, :] / self.number[                index].view(-1, 1)            self.emsemble_label[index2, :] = (1 - self.alpha) * self.emsemble_label[index2, :] + self.alpha * newlabel            return self.emsemble_label    ### easy version :to do normalization    def __call__(self, outputs, labels, idx):        with torch.no_grad():            one_hot_label = self.class2one_hot(outputs, labels)            if self.emsemble_label is None:                self.emsemble_label = torch.zeros((self.N, outputs.size(1))).cuda()            if self.is_first == True:                self.emsemble_label[idx, :] = one_hot_label            self.emsemble_label[idx, :] = \                ((1. - self.alpha) * self.emsemble_label[idx, :] + \                 self.alpha * F.softmax(outputs, dim=1)).detach()            self.emsemble_label[idx, :] = \                (1. - self.base_p) * self.emsemble_label[idx, :] + \                self.base_p * one_hot_label            return self.emsemble_label[idx, :]class Category_level_emsemble(object):    def __init__(self, Num, alpha=0.01,p=0.5,is_select=True):        self.N = Num        self.number = torch.zeros(Num,1).cuda()        self.result = torch.zeros(Num, Num).cuda()        self.emsemble_label = torch.eye(Num).cuda()        self.alpha=alpha        self.p=p        self.is_select=is_select    def reset(self):        self.number = torch.zeros(self.N, 1).cuda()        self.result = torch.zeros(self.N, self.N).cuda()    def append(self,output,target):        batch = target.size(0)        feature_num =self.N        mask = torch.zeros(batch, self.N).cuda()        mask = mask.scatter_(1, target.view(batch, 1).cuda(), 1)  # N*k 每一行是一个one-hot向量        if self.is_select==True:            _, predicted = torch.max(output.data, 1)            index = (predicted == target)            select = index.view(batch, 1).type(torch.cuda.FloatTensor)            mask = mask.type(torch.cuda.FloatTensor) * select        mask = mask.view(batch, self.N, 1) # N*k*1 目的是扩成 N*k*s        soft_logit = torch.nn.functional.softmax(output, dim=1)        output_ex = soft_logit.view(batch, 1, feature_num) # N*1*s 目的是扩成 N*k*s        sum = torch.sum(output_ex * mask, dim=0)        self.result += sum        self.number += mask.sum(dim=0)    def update(self):        index = (self.number != 0)        index2 = index.view(1, -1).squeeze()        #print(self.p*torch.eye(self.N)[index,:].cuda())        #print(self.number[index])        newlabel = self.p*torch.eye(self.N)[index2,:].cuda() + (1-self.p)* self.result[index2,:]/self.number[index].view(-1,1)        self.emsemble_label[index2,:] = (1-self.alpha)*self.emsemble_label[index2,:]+self.alpha*newlabel        return self.emsemble_labelclass Bayesian_emsemble(object):    def __init__(self, Num, alpha=0.01,p=0.5,is_select=True):        self.N = Num        self.number = torch.zeros(Num,1).cuda()        self.result = torch.zeros(Num, Num).cuda()        self.sigma_result = torch.zeros(Num, Num).cuda()        self.emsemble_label = torch.eye(Num).cuda()        self.invert_label_sigma=1e2*torch.ones(Num, Num).cuda()        self.output_sigma=torch.zeros(Num, Num).cuda()        self.alpha=torch.zeros(Num, Num).cuda()        self.p=p        self.is_select=is_select    def append(self,output,target):        batch = target.size(0)        feature_num =self.N        mask = torch.zeros(batch, self.N).cuda()        mask = mask.scatter_(1, target.view(batch, 1).cuda(), 1)  # N*k 每一行是一个one-hot向量        if self.is_select==True:            _, predicted = torch.max(output.data, 1)            index = (predicted == target)            select = index.view(batch, 1).type(torch.cuda.FloatTensor)            mask = mask.type(torch.cuda.FloatTensor) * select        mask = mask.view(batch, self.N, 1) # N*k*1 目的是扩成 N*k*s        soft_logit = torch.nn.functional.softmax(output, dim=1)        output_ex = soft_logit.view(batch, 1, feature_num) # N*1*s 目的是扩成 N*k*s        sum = torch.sum(output_ex * mask, dim=0)        count = mask.sum(dim=0)        Num = self.N        preavg = torch.zeros(Num, Num).cuda()        index = (self.number != 0)        index2 = index.view(1, -1).squeeze()        #print(self.result[index2, :].size(), self.number[index].view(-1,1).size())        if torch.sum(self.number)!=0:            preavg[index2,:] = self.result[index2,:]/self.number[index].view(-1,1)        self.result += sum        self.number += count        curavg = torch.zeros(Num, Num).cuda()        index = (self.number != 0)        index2 = index.view(1, -1).squeeze()        if torch.sum(self.number) != 0:            curavg[index2,:] = self.result[index2,:]/self.number[index2].view(-1,1)        self.sigma_result += count*(preavg-curavg)**2+torch.sum(((output_ex-curavg)**2)*mask,dim=0)    def reset(self):        self.number = torch.zeros(self.N, 1).cuda()        self.result = torch.zeros(self.N, self.N).cuda()        self.sigma_result =  torch.zeros(self.N, self.N).cuda()    def update(self):        index = (self.number != 0)        index2 = index.view(1, -1).squeeze()        self.output_sigma[index2,:] = self.sigma_result[index2,:]/self.number[index].view(-1,1)        print('\n number:',self.number)        avg = self.result[index2,:]/self.number[index].view(-1,1)        alpha = self.number[index].view(-1,1)/(self.number[index].view(-1,1)+self.output_sigma[index2,:]*self.invert_label_sigma[index2,:])        print('output_sigma:',self.output_sigma)        print('invert_label_sigma:',self.invert_label_sigma)        self.alpha[index2,:] = alpha        #print('emsemble_label_sum', torch.sum(self.emsemble_label, dim=1))        print('alpha:',self.alpha)        newlabel = self.p * torch.eye(self.N)[index2, :].cuda() +(1-self.p)*avg        self.emsemble_label[index2, :]= (1-self.alpha[index2,:])*self.emsemble_label[index2,:] + self.alpha* newlabel        #print('avg', avg)        #print('emsemble_label_sum',torch.sum(self.emsemble_label,dim=1))        self.invert_label_sigma[index2,:] = self.invert_label_sigma[index2,:] + self.number[index].view(-1,1)/ self.output_sigma[index2,:]        #print('invert_label_sigma', self.invert_label_sigma)if __name__ == '__main__':    import os    os.environ['CUDA_VISIBLE_DEVICES'] = '5'    processor = Sample_label_ensemble(5,3)    output = torch.tensor([[0.9, 0.1, 0], [0.8, 0, 0.2]]).cuda()    target = torch.tensor([0, 0]).cuda()    processor(output,target,torch.tensor([2,3]).cuda())    print(processor.emsemble_label)    #processor.append(output, target,torch.tensor([2,3]).cuda())    #print(processor.result,processor.number)    #processor.update()    '''    num = 3    processor = Bayesian_emsemble(num)    output = torch.tensor([[0.9, 0.1, 0], [0.8, 0, 0.2]]).cuda()    target = torch.tensor([0, 0]).cuda()    processor.append(output, target)    print(processor.sigma_result)    output = torch.tensor([[0.9, 0.1, 0], [0.8, 0, 0.2]]).cuda()    target = torch.tensor([0, 0]).cuda()    processor.append(output, target)    print(processor.sigma_result)    processor.update()    print(processor.alpha)    print(processor.emsemble_label)    output = torch.tensor([[0.9, 0.1, 0], [0.8, 0, 0.2]]).cuda()    target = torch.tensor([0, 0]).cuda()    processor.append(output, target)    print(processor.sigma_result)    output = torch.tensor([[0.9, 0.1, 0], [0.8, 0, 0.2]]).cuda()    target = torch.tensor([0, 0]).cuda()    processor.append(output, target)    print(processor.sigma_result)    processor.update()    print(processor.alpha)    print(processor.emsemble_label)    '''